{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from data import input_data\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=\"imagenet\")\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(nn.Linear(25088,4096),\n",
    "                                nn.ReLU6(True),\n",
    "                                nn.Dropout(0.5,inplace=False),\n",
    "                                nn.Linear(4096,4096),\n",
    "                                nn.ReLU6(True),\n",
    "                                nn.Dropout(0.5,inplace=False),\n",
    "                                nn.Linear(4096,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = []\n",
    "for i in [i for i in classifier.state_dict().keys()][:-2]:\n",
    "    f.append(((i,vgg16.classifier.state_dict()[i]))) \n",
    "for i in [i for i in classifier.state_dict().keys()][-2:]:\n",
    "    f.append(((i,classifier.state_dict()[i]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vgg16.classifier = classifier\n",
    "vgg16.classifier.load_state_dict(OrderedDict(f),strict=True)\n",
    "vgg16.to(device)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU6(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU6(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of classes in valid are :  10\n",
      "size of data 9984\n",
      "total number of classes in valid are :  10\n",
      "size of data 996\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(vgg16.parameters(), lr=0.0001)\n",
    "criterion_d = nn.CrossEntropyLoss()\n",
    "input_train = input_data(root_dir = \"/home/hemant/net/easy_net/data/train/\", type = \"valid\")\n",
    "train_dl =  DataLoader(input_train, batch_size=64,shuffle=True, num_workers=4)\n",
    "input_valid = input_data(root_dir = \"/home/hemant/net/easy_net/data/test/\", type = \"valid\")\n",
    "valid_dl =  DataLoader(input_valid, batch_size=64,shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.load_state_dict(torch.load(\"../weights/vgg16.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of epoch:  1\n",
      "[1,    25] loss: 1.913\n",
      "[1,    50] loss: 1.966\n",
      "[1,    75] loss: 2.105\n",
      "[1,   100] loss: 2.289\n",
      "[1,   125] loss: 2.240\n",
      "[1,   150] loss: 2.226\n",
      "It took :  1.1645395835240682  mins for the last training epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb232e271454d34be91f8847cdf1fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy and val loss is :  86.94779116465864 , 0.529678113758564  --AND--   It took :  7.521206617355347  seconds \n",
      "start of epoch:  2\n",
      "[2,    25] loss: 2.082\n",
      "[2,    50] loss: 1.722\n",
      "[2,    75] loss: 2.677\n",
      "[2,   100] loss: 2.986\n",
      "[2,   125] loss: 1.557\n",
      "[2,   150] loss: 1.955\n",
      "It took :  1.1477917989095052  mins for the last training epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b5679174ef44cdabc36ead079ed3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy and val loss is :  83.83534136546184 , 0.7091823238879442  --AND--   It took :  7.784949541091919  seconds \n",
      "start of epoch:  3\n",
      "[3,    25] loss: 2.359\n",
      "[3,    50] loss: 1.463\n",
      "[3,    75] loss: 0.827\n",
      "[3,   100] loss: 0.541\n",
      "[3,   125] loss: 0.752\n"
     ]
    }
   ],
   "source": [
    "stat = 0\n",
    "for j in range(50):\n",
    "    print(\"start of epoch: \", j+1)\n",
    "    #Training\n",
    "    running_loss = 0\n",
    "    start = time()\n",
    "    vgg16.train()\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "\n",
    "        input, target, img_name, number_of_class = data\n",
    "        input, target = (input.type(torch.float32)).to(device), target.to(device)\n",
    "\n",
    "        out = vgg16(input)\n",
    "\n",
    "        loss = criterion_d(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # print every 25 mini-batches\n",
    "        if i % 25 == 24:\n",
    "            print('[%d, %5d] loss: %.3f' %(j + 1, i + 1, running_loss))\n",
    "            running_loss = 0\n",
    "    end = time()\n",
    "    print(\"It took : \", (end - start)/60, \" mins for the last training epoch\")\n",
    "    \n",
    "    running_loss, acc, num, length = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        start = time()\n",
    "        for i, data in tqdm(enumerate(valid_dl, 0),total=len(valid_dl), unit=\"images\",position=0,leave=True):\n",
    "            vgg16.eval()\n",
    "\n",
    "            input, target, img_name, number_of_class = data\n",
    "            input, target = (input.type(torch.float32)).to(device), target.to(device)\n",
    "\n",
    "            out = vgg16(input)\n",
    "\n",
    "            loss = criterion_d(out, target)\n",
    "            running_loss += loss.cpu().numpy()\n",
    "            out , predicted = torch.max(out, 1)\n",
    "            for k in range(len(target)):\n",
    "                if target[k] == predicted[k].item():\n",
    "                    num = num + 1\n",
    "            length = length + len(target)\n",
    "        acc = (num/length)*100\n",
    "        end = time()\n",
    "        print(\"accuracy and val loss is : \",acc,\",\",running_loss/(i+1), \" --AND-- \", \" It took : \", (end - start), \" seconds \")\n",
    "    \n",
    "    if acc > stat:\n",
    "        stat = acc\n",
    "        torch.save(vgg16.state_dict(),\"../weights/\" + \"vgg16\" + \".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2192edce99914b4fb04297f6503eff17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy and val loss is :  87.149 , 0.567  --AND--   It took :  8.11099100112915  seconds \n"
     ]
    }
   ],
   "source": [
    "running_loss, acc, num, length = 0, 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    start = time()\n",
    "    for i, data in tqdm(enumerate(valid_dl, 0),total=len(valid_dl), unit=\"images\",position=0,leave=True):\n",
    "        vgg16.eval()\n",
    "\n",
    "        input, target, img_name, number_of_class = data\n",
    "        input, target = (input.type(torch.float32)).to(device), target.to(device)\n",
    "\n",
    "        out = vgg16(input)\n",
    "\n",
    "        loss = criterion_d(out, target)\n",
    "        running_loss+=loss.cpu().numpy()\n",
    "\n",
    "        out , predicted = torch.max(out, 1)\n",
    "        for k in range(len(target)):\n",
    "            if target[k] == predicted[k].item():\n",
    "                num = num + 1\n",
    "        length = length + len(target)\n",
    "    acc = (num/length)*100\n",
    "    end = time()\n",
    "    print(\"accuracy and val loss is : \",np.round(acc,3),\",\",np.round(running_loss/(i+1),3), \" --AND-- \", \" It took : \", (end - start), \" seconds \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(vgg16.state_dict(),\"../weights/\" + \"vgg16\" + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
